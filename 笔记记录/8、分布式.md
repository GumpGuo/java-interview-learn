## 1、分布式CAP原则？
 - CAP原则有称为CAP定理，指的是一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition tolerance(分区容错性)这3个几把呢需求，最多只能同时满足其中的两个。
 - Consistency一致性：值多个数据在多个副本之间能够保持一致的特性（严格的一致性）
 - Availability可用性：之系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应。
 - Partition tolerance(分区容错性)：分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障。

## 2、Base理论
 - Base是Basic Available（基本可用）、Soft-state（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。Base理论是对CAP中一致性和可用性A权衡的结果。
 - Base理论的核心思想：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。
1. 基本可用：指的是分布式系统出现不可预知故障的时候，允许损失部分可用性。
    - 什么叫部分可用性
        - 响应时间上的损失
        - 系统功能上的损失：正常情况下，用于可以使用系统的全部功能，但是由于系统访问量突然增加，系统的部分非核心功能无法使用。
       
2. 软状态：软状态值允许系统中的数据存在中间状态（CAP理论中的数据不一致）。并认为该状态的存在不会影响系统的整体可用性。机允许系统在不同节点的数据副本之间继续宁数据同的过程存在延时。
3. 最终一致性：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够到达一个一致的状态。因此，最终一致性的本质是需要系统保证最终数能够达到一直，而不需要实时保证系统数据的强一致性。
    - 分布式一致性的三个级别：
        - 1. 强一致性：系统写入什么，读出来就是什么
        - 2. 弱一致性：不一定可以读取到最新写入的值，也不保证多少时间后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一直的状态。
        - 3. 最终一致性：系统会保证一定时间内达到数据一致的状态。
    
##  3、分布式算法paxos

 - Paxos算法是兰伯特在1990年提出的一种分布式算法。
 - 兰伯特当时提出的Paxos算法主要包括两个部分，BasicPaxos算法和Multi-Paxos算法
 - Raft算法那、ZAB算法、FastPaxos算法都是基于Paxos算法改进而来的
      
 - 3种角色：    
     - 1.Proposer（提议者）：提议者提提案，用于投票表决。Proposal信息包括Proposal ID和提议的值（Value）。
     - 2.Acceptor（接收者）：对提案进行投票，并接受达成共识的提案。
     - 3.Learner（学习者）：被告知投票的结果，接受达成共识的提案。
 - 算法流程：Paxos算法包括两个阶段：第一个阶段Prepare（准备），第二个阶段Accept（接受）
    - 第一阶段：Prepare阶段：
       1. 提议者提出一个新的方案[Mn,?],然受向接收者的某个超过半数的子集发出编号为Mn的准备请求。
       2. Acceptors针对收到的Prepare请求进行Promise承诺。
    - 第二阶段：Accept接受阶段：
       - Proposer收到多数Acceptor的承诺的Promise，想Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理。
    - 第三阶段： Learn阶段 
       - Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners。
  - Multi-Paxos算法：
   - 原始的Paxos算法只能针对一个值进行决议。决议的形成至少需要两个网路来回。
   - Multi-Paxos算法针对Basic-Paxos算法进行了以下两点改进。
      - 1、针对每个要确定的值。运行一次Paxos算法实例，形成决议。每个Paxos实例使用唯一的Instance ID标识。
      - 2、在所有的Proposer中选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决。这样没有Proposer竞争，解决了活锁的问题。在系统仅有一个Leader的情况下。Prepare阶段就可以跳过。
  - 【参考 https://zhuanlan.zhihu.com/p/31780743 】
## 4、分布式Raft算法
 - Raft算法实现了和Paxos相同的功能，他将一致性分解为多个子问题：Leader选举、日志同步、安全性、日这压缩、成员变更等、
 - 三种角色：
    - Leader（领导者）：接收客户端的请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
    - Follower（跟随者）：接收并持久化Leader同步的日志。在Leader告诉日志可以提交后，提交体日志。
    - Candidate（候选人）：Leader选举过程中的临时角色。
 - Raft要求系统在任意时刻最多只有一个Leader。正常工作期间只有一个Leader和Follower。
 - Leader选举：Raft使用心跳触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Follower周期性地发送heatbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC 。结果有以下三种情况：
   -  赢得了多数的选票，成功选举为Leader；
   -  收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；
   -  没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。
 - 日志同步
 - 安全性
 - 日志压缩
 - 成员变更
 - 【参考文档 ：https://zhuanlan.zhihu.com/p/32052223】

    
## 5、分布式Gossip协议。
 - 又称 epidemic 协议（epidemic protocol），是基于流行病传播方式的节点或者进程之间信息交换的协议，在分布式系统中被广泛使用，比如我们可以使用 gossip 协议来确保网络中所有节点的数据一样。
 - Gossip协议：是一种允许在分布式系统中共享状态的去中心化通信协议，通过这种协议，我们可以将信息传播给网络或者集群中的所有成员。
 - 1.两种消息传播模式
    - 反熵（Anti-Entropy）和传谣（Rumor-Mongering）
    - 反熵就是指消除不同节点数据的差异，提升节点间数据的相似性，从而降低熵值。每隔时间段就随机选取某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异性，实现数据的最终一致性。
    - 在实现反熵的时候主要有推、拉和推拉三种方式：
      - 推：推方式，就是将自己的副本数据退给对方，修复对方副本中的熵。
      - 拉：拉方式就是拉取对方的所有副本数据，修复自己副本中的熵。
      - 推拉：同时修复自己副本和对方副本中的熵。
    - 谣言传播指的是分布式系统的一个节点一旦有了新数据之后，就会变成活跃节点，活跃节点会周期性的想其他节点发送新数据，直到所有的节点存储了该数据。
 - 总结：
   - 反熵会传播节点的所有数据，二谣言传播只会传播节点中新增的数据
   - 我们一般会给反熵设计一个闭环。
   - 谣言传播比较适合节点数量比较多或者节点动态变化的场景。
 - 优缺点：
   - 优势：
      - 1、相比于其他分布式算法，Gossip理解起来更简单。
      - 2、能够容忍任意节点的增加或者减少。
      - 3、速度相对较快。
   - 劣势
      - 1、消息需要经过多次传播的轮次才能到达整个网络
      - 2、不允许存在恶意节点
      - 3、可能会存在消息冗余。同一节点可能会重复收到消息。
   
## 6、分布式ID的实现方式：
### 1、什么是分布式ID?
 - 分布式ID是分布式系统下一ID。ID就是数据的唯一标识。
 
### 2、分布式ID应该满足哪些要求？
 - 全局唯一性
 - 高性能：生成速度快，本地资源占用要小。
 - 高可用：
 - 方便易用
除了这些以外
 - 安全：不包含敏感信息
 - 有序递增
 - 有具体的业务含义。
 - 独立部署

### 2、分布式ID实现方案？
 - 1、数据库自增ID   
   - 优点：实现简单
   - 缺点：支持的并发量不大。存在数据库单点问题。没有业务含义、有敏感数据（订单量）、每次获取Id都要访问数据库、 
 - 2、数据库号段模式
   - 优点：ID有序递增、存储消耗的空间小。
   - 缺点：存在数据库单点问题、安全性、ID没有具体的业务含义。
 - 3、Redis
   - 通过 Redis 的 incr 命令即可实现对 id 原子顺序递增。
   - 为了提高可用性和并发，我们可以使用 Redis Cluster。
   - 优缺点：   
    - 优点：性能不错且ID是自增的。
    - 缺点：和数据库主键自增方案的缺点类似
 - 4、算法实现
   - UUID：UUID 是 Universally Unique Identifier（通用唯一标识符） 的缩写。UUID 包含 32 个 16 进制数字（8-4-4-4-12）。不同版本对应的UUID生成规则不一样
     - 版本1：UUID 是根据时间和节点 ID（通常是 MAC 地址）生成
     - 版本2：UUID 是根据标识符（通常是组或用户 ID）、时间和节点 ID 生成；
     - 版本3、版本5：版本 5 - 确定性 UUID 通过散列（hashing）名字空间（namespace）标识符和名称生成；
     - 版本4：UUID 使用随机性或伪随机性生成。、
   - 优缺点：
      - 优点：生成速度快、简单易用
      - 缺点：消耗空间大、不安全（MAC地址泄露）、无序、不自增、没有具体的业务含义、解决重复ID（机器时间不正确）
   - 雪花算法：Snowflake 是 Twitter 开源的分布式 ID 生成算法。Snowflake 由 64 bit 的二进制数字组成，这 64bit 的二进制被分成了几部分，每一部分存储的数据都有特定的含义：
      - 第0位：符号位（标识正负），始终为0，没有用。不用管
      - 1-41位：一共 41 位，用来表示时间戳，单位是毫秒，可以支撑 2 ^41 毫秒（约 69 年）
      - 42-52位：一共 10 位，一般来说，前 5 位表示机房 ID，后 5 位表示机器 ID（实际项目中可以根据实际情况调整）。这样就可以区分不同集群/机房的节点。
      - 53-64位：一共 12 位，用来表示序列号。 序列号为自增值，代表单台机器每毫秒能够产生的最大 ID 数(2^12 = 4096),也就是说单台机器每毫秒最多可以生成 4096 个 唯一 ID。
   - 优缺点：
      - 优点：生成速度比较快、生成的 ID 有序递增、比较灵活（可以对 Snowflake 算法进行简单的改造比如加入业务 ID）
      - 缺点：需要解决重复 ID 问题（依赖时间，当机器时间不对的情况下，可能导致会产生重复 ID）。
 - 5、开源框架
   - UidGenerator(百度)
   - Leaf(美团)
   - Tinyid(滴滴) 

## 7、分布式锁的实现？
### 1、分布式锁应该具备哪些条件？
 - 1、互斥
 - 2、高可用
 - 3、可重入
 除了以上 一个好用的分布式锁还应该
 - 4、高性能
 - 5、非阻塞：如果获取不到锁，不能无限阻塞

### 2、分布式锁有哪些实现方式？
 - 1、基于关系型数据库。eg。MySQL
 - 2、基于分布式协调服务。eg.Zookeeper
 - 3、基于分布式键值存储系统 eg. Redis

### 3、基于Redis的分布式锁怎么实现？
 - SETNX命令：在Redis中。SETNX命令可以帮助我们实现互斥。SETNX即SET if Not eXists。如果key不存在的话，才会设置key的值。如果key已经存在，那么SETNX啥也不会做。
 - 释放锁的话。直接通过DEL命令删除对应的Key即可。
   - 为了防止误删了其他的锁。我们建议使用Lua脚本通过Key对应的value（唯一值）来判断，选用Lua脚本是为了保证解锁操作的原子性。
 - 给锁这是一个过期时间。
   - 为了避免锁无法被释放。我们开想到一个解决办法就是：给这个key（也就是锁）设置一个过期时间。
   - set lockKey uniqueValue EX 3 NX
    - lockKey：加锁的锁名字。
    - uniqueValue：能够唯一表示所的随机字符串
    - NX
    - EX：过期时间设置。
 - Redisson框架
 - Redisson是一个开源的Redis客户端框架。提供了很多开箱即用的功能。不仅包括各种分布式锁的实现。还支持Redis单机、Redis Sentinel、Redis Cluster等多种部署架构。
    - Redisson中的分布式锁带有自动续期机制。其提供了一个专门用来监控和续期锁的 Watch Dog（ 看门狗），如果操作共享资源的线程还未执行完成的话，Watch Dog 会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。
    - 只有未指定锁超时时间，才会用到Watch Dog自动续期机制。
 - Redis如何解决集群情况下的分布式锁的可靠性？
  - Redlock算法：算法的思想是让客户端向 Redis 集群中的多个独立的 Redis 实例依次请求申请加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。即使部分 Redis 节点出现问题，只要保证 Redis 集群中有半数以上的 Redis 节点可用，分布式锁服务就是正常的。

### 4、基于Zookeeper实现分布式锁。
 - Zookeeper分布式锁是基于临时顺序节点和Watcher（事件监听器）实现的。
  - 获取锁：
     - 1、首先我们要有个持久节点/locks，客户端获取锁就是在locks下创建临时顺序节点。
     - 2、假设客户端1创建了/locks/lock1节点，创建成功之后，会判断lock1是否是 /locks下最小的子节点。
     - 3、如果lock1是最小的子节点。则获取锁成功。否则获取锁失败。
     - 4、如果获取锁失败，则说明有其他的客户端已经成功获取锁。客户端 1 并不会不停地循环去尝试加锁，而是在前一个节点比如/locks/lock0上注册一个事件监听器。这个监听器的作用是当前一个节点释放锁之后通知客户端 1（避免无效自旋），这样客户端 1 就加锁成功了。
  - 释放锁：
      - 1、成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。
      - 2、成功获取锁的客户端在出现故障之后，对应的子节点由于是临时顺序节点，也会被自动删除，避免了锁无法被释放。
      - 3、我们前面说的事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放。
  - 实际项目中，推荐使用 Curator 来实现 ZooKeeper 分布式锁。

### 5、Zookeeper分布式锁为什么要使用临时顺序节点？
 - 每个数据节点在 ZooKeeper 中被称为 znode，它是 ZooKeeper 中数据的最小单元。
 - 我们通常是将 znode 分为 4 大类：
  - 持久（PERSISTENT）节点：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。
  - 临时（EPHEMERAL）节点：临时节点的生命周期是与 客户端会话（session） 绑定的，会话消失则节点消失 。并且，临时节点只能做叶子节点 ，不能创建子节点。
  - 持久顺序（PERSISTENT_SEQUENTIAL）节点：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 /node1/app0000000001、/node1/app0000000002 。
  - 临时顺序（EPHEMERAL_SEQUENTIAL）节点：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。

## 8、分布式事务？
 - 分布式事务有哪些常见的实现方案？
   - 分布式常见的方案有2PC、3PC、TCC、本地消息表、MQ消息事务、最大努力通知、SAGA事务等等。
### 1、2PC
 - 说到2PC就不得先说分布式事务中的XA协议。
   - 在这个协议中，有三个角色
     - AP ：应用系统（服务）
     - TM：事务管理器（全局事务管理）
     - RM：资源管理器（数据库）
   - XA协议采用两段式提交的方式管理分布式事务。XA协议提供资源管理器于事务管理器之间通信的协议标准接口。
   - 两阶段的思路可以概括为：参与者将操作成败通知协调者，再有协调者根据所有参与者的反馈情况决定各个参与者是否要提交操作还是回滚操作。
     - 准备阶段 preCommit：事务协调者要求每个涉及到事务的数据库预提交操作，执行本地事务，并反映是否可以提交。
     - 提交阶段 commit ：事务协调器要求每个数据库提交数据，或者回滚。
   - 缺点：
     - 单点问题
     - 同步阻塞
     - 数据不一致
### 2、3PC
 - 三阶段提交是二阶段的一种改进版本
   - 1、引入超时机制
   - 2、在第一个阶段和第二个阶段之间插入一个准备阶段。保证了最后提交阶段各个参与节点之间的状态是一致的。
 - 三个阶段：
   - CanCommit：准备阶段。协调者向参与制发送commit请求，参与者如果可以提交就返回yes，否则返回no响应
   - PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务。参与者执行完操作后返回ACK响应，同时开始等待最终指令。
   - DoCommit阶段：提交阶段，协调者跟根据参与者在准备阶段的响应判断是否执行事务，还是中断事务。
     - 如果所有的参与者返回正确的ACK则，提交事务
     - 如果参与者一个或多个收到错误的ACK响应或者超时，则中断事务。
     - 如果参与者无法及时收到来自协调者的提交或者中断事务请求，在等待超时过后，则继续进行事务提交。
### 3、TCC？
 - TCC是两阶段提交的一个变种，针对每一个操作，都需要有一个其对应的确认和取消操作。当操作成功是，调佣确认操作，当操作失败是调用取消操作。只不过这里的提交和回滚是针对业务上的。
 - 所以语句TCC实现的分布式事务也可以看做是对业务的一种补偿。
 - TCC的三个阶段：
   - 1、Try：尝试待进行的业务
   - 2、Config阶段：确认执行业务。
   - 3、Cancel：取消待执行的业务。
 - TCC是业务层面的分布式事务，保证最终一致性，不会一致持有锁。
   - 优点：把数据层二阶段提交交给应用层来实现，规避了数据库的2PC性能低下。
   - 缺点：TCC的try、Confirm和Cancel操作功能需提交业务功能。开发成本高。
 
### 4、本地消息表？
 - 本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。
 - 拿订单服务和库存服务为例eg执行流程
   - 1、订单服务，添加一条订单和一条消息，在一个事务里提交
   - 2、订单服务，使用定时任务轮询查询未同步的消息，发送到MQ，如果发送失败，就重新发送，
   - 3、库存服务，接受MQ消息，修改库存表，需要保证幂等性
   - 4、如果修改成功，调用rpc接口修改订单系统消息表的状态为已完成。
   - 5、如果修改失败，可以不做处理，等待重试。
### 5、MQ消息事务？
 - 消息事务的原理是将两个事务同时中间件进行解耦？
 - 订单服务执行自己的本地事务，并发送MQ消息，库存服务接收消息，执行自己的本都事务。乍一看好像跟本地消息表的实现方案一样，只是省去了本地消息表的操作和轮询发送MQ的操作。但实际上两种方案实现是不一样的。
 - 消息事务一定要保证业务操作和消息发送的一致性，如果业务操作成功，这条消息也一定投递成功。
 - 执行流程
   - 1、发送prepare消息到消息中间件。
   - 2、发送成功后，执行本地事务。
   - 3、如果事务执行成功，则commit，消息中间件会把消息发送给消费端。
   - 4、如果事务执行失败，则回滚，消息中间件将prepare消息删除。
   - 5、消费端接收到消息进行消费，如果消费失败，则不断重试。
 - 消息事务依赖消息中间件的消息事务。例如我们熟悉的RocketMQ就支持事务消息。
### 6、最大努力通知？
 - 最大努力通知相比实现会简单一些，适用一些对最终一致性实时性要求没有那么高的业务，例如支付系统
 - 执行流程：
   - 1、业务系统调用支付平台支付接口，并在本地记录，支付状态为支付中。
   - 2、支付平台进行支付后，无论成功还是失败，同步给业务系统一个结果通知。
   - 3、如果通知一直失败则会根据重试规则异步进行重试，达到最大通知次数后，不再通知。
   - 4、支付平台提供查询订单支付操作结果接口。
   - 5、业务系统根据一定的业务规则去支付平台查询支付结果。
### 7、Seata框架？
 - Seata 的设计目标是对业务无侵入，因此它是从业务无侵入的两阶段提交（全局事务）着手，在传统的两阶段上进行改进，他把一个分布式事务理解成一个包含了若干分支事务的全局事务。而全局事务的职责是协调它管理的分支事务达成一致性，要么一起成功提交，要么一起失败回滚。也就是一荣俱荣一损俱损~
 - Seata中有折磨几种重要的角色：
    - TC（Transaction Coordinator）：事务协调者。管理全局事务的运行状态，用于全局性事务的提交和回滚。
    - TM（Transaction Manager）：事控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。
    - RM（Resource Manager）:资源管理器。用于分支事务上的资源管理，向TC注册分支事务，上报分支事务的状态，接收TC的命令来条或者回滚分支事务。  
 - Seata整体执行的过程：
    - 1、服务A中的TM向TC申请开启一个全局事务。TC就会创建一个全局事务并返回一个全局的XID
    - 2、服务A中的RM向TC注册分支事务，然后将这个分支事务纳入XID对应的全局事务管辖中
    - 3、服务A开始执行分支事务。
    - 4、服务A开始执行远程调用B服务，此时XID会根据调用链传播。
    - 5、服务B中的 RM 也向 TC 注册分支事务，然后将这个分支事务纳入 XID 对应的全局事务管辖中
    - 6、服务B开始执行分支事务。
    - 7、全局事务调用处理结束后，TM会根据有无异常的情况，向TC发起全局事务的提交或者恢复
    - 8、TC协调其管辖之下的所有分支事务，决定提交还是回滚。
 - Seata整体执行流程：
  - 1、TM向TC申请一个全局事务，TC创建全局事务后返回全局唯一的XID，XID会在全局事务的上下文中传播。
  - 2、RM向TC注册分支事务。该分支事务归属于拥有相同XID的全局事务。
  - 3、TM向TC发起全局提交或者回滚。
  - 4、TC调度XID下的分支事务完成提交或者回滚。
